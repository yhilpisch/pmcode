{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "d30d53cbd6fd448783d00eea9446a284"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Mathematics for Data Science and Machine Learning\n",
    "\n",
    "**© Dr. Yves J. Hilpisch | The Python Quants GmbH**<br>\n",
    "AI-powered by GPT-5.\n",
    "\n"
   ],
   "id": "0cfdae82ae614b8e85ddf93f9a55c800"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure plotting and a reproducible RNG for consistent results.\n"
   ],
   "id": "d98dc0d6b67f4edca186f8566d0ecc8a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 — Why Math With Code (and Code With Math)\n",
    "\n",
    "This notebook mirrors the first chapter. It demonstrates the ‘Math ↔ Code’ loop: state a claim, test it with a small experiment, and refine intuition from the results.\n",
    "\n",
    "You’ll Learn\n",
    "\n",
    "- Set up tiny numerical experiments to verify a claim\n",
    "- Use seeds and sanity checks for reproducibility\n",
    "- Translate a statement into code and back into math\n"
   ],
   "id": "201c491ccf54452698b151e482d14ddf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, plotting style, and reproducible RNG\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np  # numerical arrays and linear algebra\n",
    "import matplotlib.pyplot as plt  # plotting library\n",
    "plt.style.use('seaborn-v0_8')  # consistent plot style\n",
    "# Fixed seed for reproducible random numbers across runs\n",
    "rs = np.random.default_rng(42)  # reproducible random generator\n"
   ],
   "id": "c407e9cb0e2248cb82893648945b8804"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of Large Numbers (LLN): quick numerical check\n",
    "\n",
    "We expect the sample mean to approach the true mean as sample size grows (under standard assumptions). We’ll probe this behavior numerically."
   ],
   "id": "d7c37807644b41408d8624ba572b2b40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lln_demo(dist, n_values=(100, 1_000, 10_000)):  # LLN demo\n",
    "    \"\"\"Check how the sample mean approaches the expectation.\n",
    "    dist: small dict with 'mean' and 'gen'(rng,size)->array\n",
    "    \"\"\"\n",
    "    mu = dist['mean']  # theoretical mean of distribution\n",
    "    for n in n_values:  # iterate over sample sizes\n",
    "        x = dist['gen'](rs, size=n)  # draw samples via provided generator\n",
    "        m = x.mean()  # sample mean\n",
    "        err = abs(m - mu)  # absolute error to true mean\n",
    "        print(f\"n={n:>6d}  mean={m:+.6f}  |mean-mu|={err:.6f}\")  # report results\n",
    "        # Sanity: finite outputs\n",
    "        assert np.isfinite(m)\n",
    "\n",
    "# Standard Normal: E[X] = 0\n",
    "normal = {  # specify standard normal distribution\n",
    "    'mean': 0.0,  # true mean\n",
    "    'gen': lambda rng, size: rng.standard_normal(size=size).astype(\n",
    "        np.float64\n",
    "    ),  # generator for N(0,1)\n",
    "}\n",
    "\n",
    "lln_demo(normal)\n"
   ],
   "id": "51fb6af4d95c4653bb74c17892878f27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance non-negativity as a falsifiable check\n",
    "\n",
    "We test the identity $\\operatorname{Var}(X)=\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\ge 0$ numerically and allow for tiny negative estimates due to rounding (finite samples, `float64`)."
   ],
   "id": "638308f3d16040c8862ca272ca2e2435"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_nonneg_demo(rng, n=50_000):  # demo: Var(X) ≥ 0 numerically\n",
    "    # Draw samples and estimate E[X], E[X^2], and Var~ = E[X^2] - (E[X])^2\n",
    "    x = rng.standard_normal(size=n).astype(np.float64)  # draw normal samples\n",
    "    ex = x.mean()  # estimate E[X]\n",
    "    ex2 = (x * x).mean()  # estimate E[X^2]\n",
    "    var_est = ex2 - ex * ex  # Var~ = E[X^2] - (E[X])^2\n",
    "    print(\n",
    "        f\"E[X]={ex:+.4f}, E[X^2]={ex2:+.4f}, Var~={var_est:+.6f}\"\n",
    "    )  # report results\n",
    "    assert var_est > -1e-12  # allow tiny negatives from rounding\n",
    "\n",
    "variance_nonneg_demo(rs)\n"
   ],
   "id": "78bf4624e2374a5a996c098553ad6a2a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: running mean stabilizes with n\n",
    "\n",
    "We plot the running sample mean $\\bar{X}_n$ against sample size on a logarithmic x-axis. Expect large swings for tiny n and gradual stabilization around 0."
   ],
   "id": "3a8b1a3945544a5aa74df3afe297b350"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20_000  # number of samples\n",
    "x = rs.standard_normal(size=N).astype(np.float64)  # draw samples for visualization\n",
    "running_mean = np.cumsum(x) / np.arange(1, N + 1)  # cumulative average $\\bar X_n$\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6.8, 3.6), dpi=140)  # create figure and axes\n",
    "ax.plot(  # plot running mean vs. n\n",
    "    np.arange(1, N + 1),  # x: sample index\n",
    "    running_mean,          # y: running mean\n",
    "    color='C0', lw=1.6, label='running mean'  # style\n",
    ")\n",
    "ax.axhline(  # reference line at true mean 0\n",
    "    0.0, color='k', lw=1.0, ls='--',  # style\n",
    "    label=r'true mean $\\mu=0$'\n",
    ")\n",
    "ax.set_xscale('log')  # logarithmic x-axis\n",
    "ax.set_xlabel('n (log scale)')  # label x-axis\n",
    "ax.set_ylabel(r'sample mean $\\bar X_n$')  # label y-axis\n",
    "ax.set_title('LLN in action: running mean vs. sample size')  # add plot title\n",
    "ax.legend(loc='best')  # show legend\n",
    "ax.grid(alpha=0.25)  # light grid for readability\n",
    "plt.show()  # render the figure\n"
   ],
   "id": "5e4db324b02c4caf8901374c8ac09490"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "- Early samples dominate: small n yields large swings.\n",
    "- Averaging tames noise: typical error shrinks roughly like 1/√n (light tails).\n",
    "- Randomness persists: different seeds produce different paths with the same pattern.\n"
   ],
   "id": "fbbc30414f924f17973955f2c1d00b17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure Generators (for reproducibility)\n",
    "\n",
    "- `code/figures/ch01_lln_running_mean.py` — running mean (LLN).\n"
   ],
   "id": "d3fc8f4827834090a32cc4528caec2bb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "da23ccd3508f4a3fb84314ba039001b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}