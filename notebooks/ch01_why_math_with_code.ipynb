{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n</p>"
   ],
   "id": "d30d53cbd6fd448783d00eea9446a284"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Mathematics for Data Science and Machine Learning\n\n(c) Dr. Yves J. Hilpisch | The Python Quants GmbH\n\nAI-powered by GPT-5\n"
   ],
   "id": "0cfdae82ae614b8e85ddf93f9a55c800"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 — Why Math With Code (and Code With Math)\n\n",
    "This notebook mirrors the first chapter. It demonstrates the ‘Math ↔ Code’ loop: state a claim, test it with a small experiment, and refine intuition from the results.\n\n",
    "You’ll Learn\n\n",
    "- Set up tiny numerical experiments to verify a claim\n",
    "- Use seeds and sanity checks for reproducibility\n",
    "- Translate a statement into code and back into math\n"
   ],
   "id": "201c491ccf54452698b151e482d14ddf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, plotting style, and reproducible RNG\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "# Fixed seed for reproducible random numbers across runs\n",
    "rs = np.random.default_rng(42)\n"
   ],
   "id": "c407e9cb0e2248cb82893648945b8804"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Law of Large Numbers (LLN): quick numerical check\n\n",
    "We expect the sample mean to approach the true mean as sample size grows (under standard assumptions). We’ll probe this behavior numerically."
   ],
   "id": "d7c37807644b41408d8624ba572b2b40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lln_demo(dist, n_values=(100, 1_000, 10_000)):\n",
    "    \"\"\"Check how the sample mean approaches the expectation.\n",
    "    dist: small dict with 'mean' and 'gen'(rng,size)->array\n",
    "    \"\"\"\n",
    "    mu = dist['mean']\n",
    "    for n in n_values:\n",
    "        x = dist['gen'](rs, size=n)\n",
    "        m = x.mean()\n",
    "        err = abs(m - mu)\n",
    "        print(f\"n={n:>6d}  mean={m:+.6f}  |mean-mu|={err:.6f}\")\n",
    "        # Sanity: finite outputs\n",
    "        assert np.isfinite(m)\n",
    "\n",
    "# Standard Normal: E[X] = 0\n",
    "normal = {\n",
    "    'mean': 0.0,\n",
    "    'gen': lambda rng, size: rng.standard_normal(size=size).astype(np.float64),\n",
    "}\n",
    "\n",
    "lln_demo(normal)\n"
   ],
   "id": "51fb6af4d95c4653bb74c17892878f27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance non-negativity as a falsifiable check\n\n",
    "We test the identity Var(X) = E[X^2] − (E[X])^2 ≥ 0 numerically and allow for tiny negative estimates due to rounding (finite samples, float64)."
   ],
   "id": "638308f3d16040c8862ca272ca2e2435"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_nonneg_demo(rng, n=50_000):\n",
    "    # Draw samples and estimate E[X], E[X^2], and Var~ = E[X^2] - (E[X])^2\n",
    "    x = rng.standard_normal(size=n).astype(np.float64)\n",
    "    ex = x.mean()\n",
    "    ex2 = (x * x).mean()\n",
    "    var_est = ex2 - ex * ex\n",
    "    print(f\"E[X]={ex:+.4f}, E[X^2]={ex2:+.4f}, Var~={var_est:+.6f}\")\n",
    "    assert var_est > -1e-12  # allow tiny negatives from rounding\n",
    "\n",
    "variance_nonneg_demo(rs)\n"
   ],
   "id": "78bf4624e2374a5a996c098553ad6a2a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: running mean stabilizes with n\n\n",
    "We plot the running sample mean \\bar{X}_n against sample size on a logarithmic x-axis. Expect large swings for tiny n and gradual stabilization around 0."
   ],
   "id": "3a8b1a3945544a5aa74df3afe297b350"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20_000\n",
    "x = rs.standard_normal(size=N).astype(np.float64)\n",
    "running_mean = np.cumsum(x) / np.arange(1, N + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6.8, 3.6), dpi=140)\n",
    "ax.plot(np.arange(1, N + 1), running_mean, color='C0', lw=1.6, label='running mean')\n",
    "ax.axhline(0.0, color='k', lw=1.0, ls='--', label=r'true mean $\\mu=0$')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('n (log scale)')\n",
    "ax.set_ylabel(r'sample mean $\\bar X_n$')\n",
    "ax.set_title('LLN in action: running mean vs. sample size')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(alpha=0.25)\n",
    "plt.show()\n"
   ],
   "id": "5e4db324b02c4caf8901374c8ac09490"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n\n",
    "- Early samples dominate: small n yields large swings.\n",
    "- Averaging tames noise: typical error shrinks roughly like 1/√n (light tails).\n",
    "- Randomness persists: different seeds produce different paths with the same pattern.\n"
   ],
   "id": "fbbc30414f924f17973955f2c1d00b17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure Generators (for reproducibility)\n\n",
    "- `code/figures/ch01_lln_running_mean.py` — running mean (LLN).\n"
   ],
   "id": "d3fc8f4827834090a32cc4528caec2bb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "da23ccd3508f4a3fb84314ba039001b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
