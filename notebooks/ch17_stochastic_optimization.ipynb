{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "02ea4ed1d5ae4595a820823ffcc50f09"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Mathematics for Data Science and Machine Learning\n",
    "\n",
    "**© Dr. Yves J. Hilpisch | The Python Quants GmbH**<br>\n",
    "AI-powered by GPT-5.\n",
    "\n",
    "**© Dr. Yves J. Hilpisch | The Python Quants GmbH**<br>\n",
    "AI-powered by GPT-5.\n"
   ],
   "id": "ea90c06794af4c939dadbb40a2be7874"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17 — Stochastic Optimization\n\nThis notebook mirrors the chapter’s key demos: full-batch vs SGD vs momentum on logistic regression, and mini-batch gradient unbiasedness/variance checks."
   ],
   "id": "01e841a9fb5347d49e860143bbafc5e8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np  # arrays, RNG\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "plt.style.use('seaborn-v0_8')  # house style\n",
    "rng = np.random.default_rng(17)  # reproducibility seed\n"
   ],
   "id": "b69a2809c806434192de442367ddfc0e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and helpers"
   ],
   "id": "ec850b7eea8741609bfc22fea4de8194"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_linsep(n=400, d=2):  # synthetic data generator\n",
    "    X = rng.normal(size=(n, d)).astype(float)  # features ~ N(0, I)\n",
    "    w_true = np.array([2.0, -1.0])  # ground truth\n",
    "    logits = X @ w_true + 0.25 * rng.normal(size=n)  # noisy scores\n",
    "    y = (logits > 0).astype(int)  # labels {0,1}\n",
    "    return X, y  # dataset\n",
    "\n",
    "def sigmoid(z):  # logistic link\n",
    "    return 1.0 / (1.0 + np.exp(-z))  # σ(z)\n",
    "\n",
    "def loss_batch(w, Xb, yb):  # mean log-loss\n",
    "    z = Xb @ w  # scores\n",
    "    return float(np.mean(np.log1p(np.exp(z)) - yb * z))  # stable loss\n",
    "\n",
    "def grad_batch(w, Xb, yb):  # gradient\n",
    "    z = Xb @ w  # scores\n",
    "    p = sigmoid(z)  # probabilities\n",
    "    return (Xb.T @ (p - yb)) / Xb.shape[0]  # mean gradient\n",
    "\n",
    "X, y = make_linsep()  # build dataset\n",
    "print(X.shape, y.shape, y.mean())  # shapes and class balance\n"
   ],
   "id": "0684b1d5d14d42cd96f5d4491103e545"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training utilities"
   ],
   "id": "bc6b3ac643d8461aaf8de7b5bb47df38"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(w0, X, y, epochs=40, lr=0.1, batch=None, momentum=0.0):  # trainer\n",
    "    w = w0.copy().astype(float)  # working params\n",
    "    n = X.shape[0]  # samples\n",
    "    v = np.zeros_like(w)  # momentum buffer\n",
    "    his = []  # (epoch, loss)\n",
    "    for e in range(epochs):  # epochs\n",
    "        if batch is None:  # full batch\n",
    "            g = grad_batch(w, X, y)  # full gradient\n",
    "            w = w - lr * g  # update\n",
    "        else:  # SGD\n",
    "            idx = rng.permutation(n)  # shuffle\n",
    "            for i in range(0, n, batch):  # batches\n",
    "                b = idx[i:i+batch]  # batch slice\n",
    "                g = grad_batch(w, X[b], y[b])  # batch grad\n",
    "                v = momentum * v + g  # momentum\n",
    "                w = w - lr * v  # step\n",
    "        his.append((e, loss_batch(w, X, y)))  # track loss\n",
    "    return w, np.array(his)  # (params, history)\n"
   ],
   "id": "e69e28afd8ba40e7a1bc211065a37bcf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs and results"
   ],
   "id": "af4437e9cc2d4bc0bee4cacb1a4ca161"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "w0 = np.zeros(X.shape[1])  # init\n",
    "_, H_full = train(w0, X, y, epochs=40, lr=0.5, batch=None)  # full GD\n",
    "_, H_sgd  = train(w0, X, y, epochs=40, lr=0.1, batch=32)  # SGD\n",
    "_, H_mom  = train(w0, X, y, epochs=40, lr=0.1, batch=32, momentum=0.9)  # SGD+mom\n",
    "print('final_losses:', H_full[-1,1], H_sgd[-1,1], H_mom[-1,1])  # summary\n"
   ],
   "id": "2a3353c0653b406688c387ca0ade3d55"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbiasedness and variance"
   ],
   "id": "ea0e06ce1fb84e6ab1c91161bb13fbcb"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grad_stats(w, X, y, B, trials=200):  # E[g], Var[g]\n",
    "    n = X.shape[0]  # samples\n",
    "    G = []  # collect gradients\n",
    "    for _ in range(trials):  # trials\n",
    "        b = rng.choice(n, size=B, replace=False)  # batch idx\n",
    "        G.append(grad_batch(w, X[b], y[b]))  # grad sample\n",
    "    G = np.stack(G)  # (trials, d)\n",
    "    return G.mean(0), G.var(0)  # mean, variance\n",
    "\n",
    "w_ref = np.zeros(X.shape[1])  # probe near origin\n",
    "g_full = grad_batch(w_ref, X, y)  # full gradient\n",
    "for B in (8, 16, 32, 64, 128):  # batch sizes\n",
    "    g_mean, g_var = grad_stats(w_ref, X, y, B)  # stats\n",
    "    print(f'B={B:>3} unbiased~{np.allclose(g_mean, g_full, atol=5e-3)} var_sum={g_var.sum():.3e}')  # summary\n"
   ],
   "id": "16bc6494127c4c6093ad93efd8d3f22c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "932bb52cc91542d991955dac705d3acc"
  }
 ]
}