{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0000",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0001",
   "metadata": {},
   "source": [
    "# Python & Mathematics for Data Science and Machine Learning\n",
    "\n",
    "**© Dr. Yves J. Hilpisch | The Python Quants GmbH**<br>\n",
    "AI-powered by GPT-5.\n",
    "\n",
    "**© Dr. Yves J. Hilpisch | The Python Quants GmbH**<br>\n",
    "AI-powered by GPT-5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0002",
   "metadata": {},
   "source": [
    "# Chapter 13 — Bayes & Information\n\nThis notebook mirrors the chapter. We update beliefs with data (Bayes), then measure uncertainty and mismatch (entropy, cross-entropy, KL). Small, runnable examples keep intuition honest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "plt.style.use('seaborn-v0_8')\n",
    "rs = np.random.default_rng(13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0004",
   "metadata": {},
   "source": [
    "## Beta–Bernoulli updating\n\nWe start with a mild prior Beta(2,2). Draw Bernoulli data with true probability p_true and observe how the posterior concentrates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_posterior(alpha: float, beta: float, k: int, n: int):\n",
    "    \"\"\"Return posterior parameters (alpha', beta') for Beta-Bernoulli.\n",
    "    Prior: Beta(alpha, beta). Data: k successes in n trials.\n",
    "    \"\"\"\n",
    "    return alpha + k, beta + (n - k)\n",
    "\n",
    "p_true = 0.3\n",
    "alpha0, beta0 = 2.0, 2.0\n",
    "n = 100\n",
    "x = rs.binomial(1, p_true, size=n).astype(int)\n",
    "k = int(x.sum())\n",
    "a1, b1 = beta_posterior(alpha0, beta0, k, n)\n",
    "print(f'k={k}, posterior=Beta({a1},{b1})')\n",
    "assert 0 <= k <= n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot: prior vs posterior density\n",
    "import math\n",
    "def log_beta(a: float, b: float) -> float:\n",
    "    return math.lgamma(a) + math.lgamma(b) - math.lgamma(a + b)\n",
    "\n",
    "def beta_pdf(x: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    x = np.clip(x, 1e-12, 1 - 1e-12)\n",
    "    lb = log_beta(a, b)\n",
    "    return np.exp((a-1)*np.log(x) + (b-1)*np.log(1-x) - lb)\n",
    "\n",
    "xs = np.linspace(0.0, 1.0, 500)\n",
    "fig, ax = plt.subplots(figsize=(6.4, 3.6))\n",
    "ax.plot(xs, beta_pdf(xs, alpha0, beta0), lw=1.8, label=f'Prior Beta({alpha0:.0f},{beta0:.0f})')\n",
    "ax.plot(xs, beta_pdf(xs, a1, b1), lw=2.2, label=f'Posterior Beta({a1},{b1})')\n",
    "ax.axvline(p_true, ls='--', color='#e84855', lw=1.4, label='p_true')\n",
    "ax.set_xlabel('p')\n",
    "ax.set_ylabel('density')\n",
    "ax.legend(loc='best')\n",
    "ax.set_title('Beta–Bernoulli: prior vs posterior')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0007",
   "metadata": {},
   "source": [
    "## Entropy and KL (Kullback–Leibler) divergence\n\nFor Bernoulli, entropy is $H(p)=-(p\\\\log p + (1-p)\\\\log(1-p))$ and KL is $D_{\\\\mathrm{KL}}(p\\\\Vert q)=p\\\\log\\\\tfrac{p}{q} + (1-p)\\\\log\\\\tfrac{1-p}{1-q}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_bernoulli(p):\n",
    "    p = np.clip(p, 1e-12, 1-1e-12)\n",
    "    return -(p*np.log(p) + (1-p)*np.log(1-p))\n",
    "\n",
    "def KL_bernoulli(p, q):\n",
    "    p = np.clip(p, 1e-12, 1-1e-12)\n",
    "    q = np.clip(q, 1e-12, 1-1e-12)\n",
    "    return p*np.log(p/q) + (1-p)*np.log((1-p)/(1-q))\n",
    "\n",
    "p = np.linspace(0.0, 1.0, 400)\n",
    "fig, ax = plt.subplots(figsize=(6.0, 3.2))\n",
    "ax.plot(p, H_bernoulli(p), lw=2.0)\n",
    "ax.set_title('Entropy H(p) for Bernoulli')\n",
    "ax.set_xlabel('p')\n",
    "ax.set_ylabel('nats')\n",
    "ax.axvline(0.5, ls='--', color='#888', lw=1.0)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Small KL table for intuition\n",
    "pairs = [(0.1, 0.2), (0.1, 0.9), (0.3, 0.28), (0.9, 0.1)]\n",
    "for pp, qq in pairs:\n",
    "    print(f'D_KL(Ber({pp}) || Ber({qq})) = {KL_bernoulli(np.array([pp]), np.array([qq]))[0]:.4f} nats')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0009",
   "metadata": {},
   "source": [
    "## Cross-entropy (log-loss) check\n\nAverage negative log-likelihood equals cross-entropy for Bernoulli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_empirical(y, q):\n",
    "    q = np.clip(q, 1e-12, 1-1e-12)\n",
    "    return np.mean(-(y*np.log(q) + (1-y)*np.log(1-q)))\n",
    "\n",
    "def cross_entropy_param(p, q):\n",
    "    p = np.clip(p, 1e-12, 1-1e-12)\n",
    "    q = np.clip(q, 1e-12, 1-1e-12)\n",
    "    return -(p*np.log(q) + (1-p)*np.log(1-q))\n",
    "\n",
    "p_true, q_model, n = 0.3, 0.28, 20000\n",
    "y = rs.binomial(1, p_true, size=n)\n",
    "nll = cross_entropy_empirical(y, q_model)\n",
    "xent = cross_entropy_param(y.mean(), q_model)\n",
    "print(f'mean NLL ~ {nll:.5f}, cross-entropy(p_hat,q) ~ {xent:.5f}')\n",
    "assert abs(nll - xent) < 5e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "1b90772eb34e40eba56cb9570cf572f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}