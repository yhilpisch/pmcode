{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0000",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0001",
   "metadata": {},
   "source": [
    "# Python & Mathematics for Data Science and Machine Learning\n",
    "\n",
    "**© Dr. Yves J. Hilpisch | The Python Quants GmbH**<br>\n",
    "AI-powered by GPT-5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0002",
   "metadata": {},
   "source": [
    "# Chapter 13 — Bayes & Information\n\nThis notebook mirrors the chapter. We update beliefs with data (Bayes), then measure uncertainty and mismatch (entropy, cross-entropy, KL). Small, runnable examples keep intuition honest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up imports and basic configuration.\n"
   ],
   "id": "4906b1aee8aa42b7a35abd71b8ac4fb1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np  # numerical arrays and linear algebra\n",
    "import matplotlib.pyplot as plt  # plotting library\n",
    "from matplotlib.colors import LogNorm  # plotting utilities\n",
    "plt.style.use('seaborn-v0_8')\n",
    "rs = np.random.default_rng(13)  # reproducible random generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0004",
   "metadata": {},
   "source": [
    "## Beta–Bernoulli updating\n\nWe start with a mild prior Beta(2,2). Draw Bernoulli data with true probability p_true and observe how the posterior concentrates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function for clarity.\n"
   ],
   "id": "25f2afaab8ce4f23a6455b971c06c181"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_posterior(alpha: float, beta: float, k: int, n: int):  # beta_posterior\n",
    "    \"\"\"Return posterior parameters (alpha', beta') for Beta-Bernoulli.\n",
    "    Prior: Beta(alpha, beta). Data: k successes in n trials.\n",
    "    \"\"\"\n",
    "    return alpha + k, beta + (n - k)\n",
    "\n",
    "p_true = 0.3\n",
    "alpha0, beta0 = 2.0, 2.0\n",
    "n = 100\n",
    "x = rs.binomial(1, p_true, size=n).astype(int)\n",
    "k = int(x.sum())\n",
    "a1, b1 = beta_posterior(alpha0, beta0, k, n)\n",
    "print(f'k={k}, posterior=Beta({a1},{b1})')  # report results\n",
    "assert 0 <= k <= n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results to visualize behavior.\n"
   ],
   "id": "44ab541fc06f4ab1bf6c9891ee0e4e8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot: prior vs posterior density\n",
    "import math\n",
    "def log_beta(a: float, b: float) -> float:  # function log_beta\n",
    "    return math.lgamma(a) + math.lgamma(b) - math.lgamma(a + b)\n",
    "\n",
    "def beta_pdf(x: np.ndarray, a: float, b: float) -> np.ndarray:  # function beta_pdf\n",
    "    x = np.clip(x, 1e-12, 1 - 1e-12)\n",
    "    lb = log_beta(a, b)\n",
    "    return np.exp((a-1)*np.log(x) + (b-1)*np.log(1-x) - lb)\n",
    "\n",
    "xs = np.linspace(0.0, 1.0, 500)\n",
    "fig, ax = plt.subplots(figsize=(6.4, 3.6))\n",
    "ax.plot(\n",
    "    xs,\n",
    "    beta_pdf(xs, alpha0, beta0),\n",
    "    lw=1.8,\n",
    "    label=f\"Prior Beta({alpha0:.0f},{beta0:.0f})\"\n",
    ")  # plot element\n",
    "ax.plot(\n",
    "    xs,\n",
    "    beta_pdf(xs, a1, b1),\n",
    "    lw=2.2,\n",
    "    label=f\"Posterior Beta({a1},{b1})\"\n",
    ")  # plot element\n",
    "ax.axvline(p_true, ls='--', color='#e84855', lw=1.4, label='p_true')\n",
    "ax.set_xlabel('p')  # set axis/scale\n",
    "ax.set_ylabel('density')  # set axis/scale\n",
    "ax.legend(loc='best')  # plot element\n",
    "ax.set_title('Beta–Bernoulli: prior vs posterior')  # set axis/scale\n",
    "fig.tight_layout()\n",
    "plt.show()  # render figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0007",
   "metadata": {},
   "source": [
    "## Entropy and KL (Kullback–Leibler) divergence\n\nFor Bernoulli, entropy is $H(p)=-(p\\\\log p + (1-p)\\\\log(1-p))$ and KL is $D_{\\\\mathrm{KL}}(p\\\\Vert q)=p\\\\log\\\\tfrac{p}{q} + (1-p)\\\\log\\\\tfrac{1-p}{1-q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function for clarity.\n"
   ],
   "id": "8ad145f04f154414a40eed88bdfc6a84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_bernoulli(p):  # function H_bernoulli\n",
    "    p = np.clip(p, 1e-12, 1-1e-12)\n",
    "    return -(p*np.log(p) + (1-p)*np.log(1-p))\n",
    "\n",
    "def KL_bernoulli(p, q):  # function KL_bernoulli\n",
    "    p = np.clip(p, 1e-12, 1-1e-12)\n",
    "    q = np.clip(q, 1e-12, 1-1e-12)\n",
    "    return p*np.log(p/q) + (1-p)*np.log((1-p)/(1-q))\n",
    "\n",
    "p = np.linspace(0.0, 1.0, 400)\n",
    "fig, ax = plt.subplots(figsize=(6.0, 3.2))\n",
    "ax.plot(\n",
    "    xs,\n",
    "    beta_pdf(xs, alpha0, beta0),\n",
    "    lw=1.8,\n",
    "    label=f\"Prior Beta({alpha0:.0f},{beta0:.0f})\"\n",
    ")  # plot element\n",
    "ax.plot(\n",
    "    xs,\n",
    "    beta_pdf(xs, a1, b1),\n",
    "    lw=2.2,\n",
    "    label=f\"Posterior Beta({a1},{b1})\"\n",
    ")  # plot element\n",
    "ax.axvline(0.5, ls='--', color='#888', lw=1.0)\n",
    "fig.tight_layout()\n",
    "plt.show()  # render figure\n",
    "\n",
    "# Small KL table for intuition\n",
    "pairs = [(0.1, 0.2), (0.1, 0.9), (0.3, 0.28), (0.9, 0.1)]\n",
    "for pp, qq in pairs:\n",
    "    print(\n",
    "        f'D_KL(Ber({pp}) || Ber({qq})) =',\n",
    "        f' {KL_bernoulli(np.array([pp]), np.array([qq]))[0]:.4f} nats'\n",
    "    )  # report results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0009",
   "metadata": {},
   "source": [
    "## Cross-entropy (log-loss) check\n\nAverage negative log-likelihood equals cross-entropy for Bernoulli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function for clarity.\n"
   ],
   "id": "f819abfd7aa04ad58ff32b40bd0e67dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_empirical(y, q):  # function cross_entropy_empirical\n",
    "    q = np.clip(q, 1e-12, 1-1e-12)\n",
    "    return np.mean(-(y*np.log(q) + (1-y)*np.log(1-q)))\n",
    "\n",
    "def cross_entropy_param(p, q):  # function cross_entropy_param\n",
    "    p = np.clip(p, 1e-12, 1-1e-12)\n",
    "    q = np.clip(q, 1e-12, 1-1e-12)\n",
    "    return -(p*np.log(q) + (1-p)*np.log(1-q))\n",
    "\n",
    "p_true, q_model, n = 0.3, 0.28, 20000\n",
    "y = rs.binomial(1, p_true, size=n)\n",
    "nll = cross_entropy_empirical(y, q_model)\n",
    "xent = cross_entropy_param(y.mean(), q_model)\n",
    "print(f'mean NLL ~ {nll:.5f}, cross-entropy(p_hat,q) ~ {xent:.5f}')  # report results\n",
    "assert abs(nll - xent) < 5e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "1b90772eb34e40eba56cb9570cf572f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}