{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "61caf8f9839f47cdb47fd1aa37a58e27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Mathematics for Data Science and Machine Learning\n",
    "\n",
    "**© Dr. Yves J. Hilpisch | The Python Quants GmbH**<br>\n",
    "AI-powered by GPT-5.x.\n",
    "\n"
   ],
   "id": "5bdc5eca1100487eb6f5cc45f93b3a7c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 — Unconstrained Optimization\n\nThis notebook mirrors the chapter’s key demos: GD paths on an anisotropic quadratic, numerical checks for the descent lemma and linear rate, and a 1D Armijo backtracking slice."
   ],
   "id": "fc72ff12993b4eea999ade0901801351"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np  # arrays, RNG\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "plt.style.use('seaborn-v0_8')  # house style\n",
    "rng = np.random.default_rng(15)  # seed\n"
   ],
   "id": "abe95feed88640278316f4d7c9bf75c5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anisotropic quadratic: GD paths"
   ],
   "id": "5e13988ab46f4905a715531991e41da5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_Q(L=50.0, m=1.0, theta_deg=30.0):  # SPD with rotation\n",
    "    t = np.deg2rad(theta_deg)  # radians\n",
    "    R = np.array([[np.cos(t), -np.sin(t)], [np.sin(t), np.cos(t)]])  # rotation\n",
    "    return R.T @ np.diag([L, m]) @ R  # rotated diag\n",
    "\n",
    "def f_quad(x, Q):  # 0.5 x^T Q x\n",
    "    return 0.5 * float(x @ Q @ x)\n",
    "\n",
    "def g_quad(x, Q):  # ∇f = Q x\n",
    "    return Q @ x\n",
    "\n",
    "def backtrack(x, Q, alpha0=0.2, beta=0.5, c=1e-4):  # Armijo backtracking\n",
    "    g = g_quad(x, Q); t = alpha0; fx = f_quad(x, Q); gg = float(g @ g)\n",
    "    while f_quad(x - t*g, Q) > fx - c*t*gg:  # sufficient decrease\n",
    "        t *= beta  # shrink\n",
    "    return t  # accepted step\n",
    "\n",
    "def gd_path(Q, x0, steps, alpha=None):  # fixed step or backtracking\n",
    "    x = x0.astype(float).copy(); xs = [x.copy()]\n",
    "    for _ in range(steps):\n",
    "        g = g_quad(x, Q)  # gradient\n",
    "        t = alpha if alpha is not None else backtrack(x, Q)  # step size\n",
    "        x = x - t*g  # update\n",
    "        xs.append(x.copy())  # record\n",
    "    return np.array(xs)\n",
    "\n",
    "Q = make_Q(); x0 = np.array([2.0, -1.0])  # test bowl\n",
    "p_fix = gd_path(Q, x0, steps=28, alpha=0.03)  # fixed step\n",
    "p_bt = gd_path(Q, x0, steps=28, alpha=None)  # backtracking\n",
    "\n",
    "# contours covering both paths\n",
    "xs = np.linspace(min(p_fix[:,0].min(), p_bt[:,0].min())-0.5,\n",
    "                max(p_fix[:,0].max(), p_bt[:,0].max())+0.5, 220)\n",
    "ys = np.linspace(min(p_fix[:,1].min(), p_bt[:,1].min())-0.5,\n",
    "                max(p_fix[:,1].max(), p_bt[:,1].max())+0.5, 220)\n",
    "X, Y = np.meshgrid(xs, ys)\n",
    "Z = 0.5*(Q[0,0]*X**2 + 2*Q[0,1]*X*Y + Q[1,1]*Y**2)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4), sharex=True, sharey=True)\n",
    "for a, path, col, title in [\n",
    "    (ax[0], p_fix, '#e84855', 'Fixed step'),\n",
    "    (ax[1], p_bt, '#1b998b', 'Backtracking'),\n",
    "]:\n",
    "    a.contour(X, Y, Z, levels=12, cmap='viridis')  # contours\n",
    "    a.plot(path[:,0], path[:,1], 'o-', color=col, ms=3)  # path\n",
    "    a.plot([x0[0]], [x0[1]], 'o', color='k', ms=6)  # start\n",
    "    a.set_title(title)  # title\n",
    "    a.set_xlabel('$x_1$')  # label\n",
    "ax[0].set_ylabel('$x_2$')  # y label\n",
    "plt.tight_layout(); plt.show()  # render\n"
   ],
   "id": "2705a1ef27884d6a892dd43044945b94"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descent lemma and linear-rate checks (quadratic)"
   ],
   "id": "09c1c2b28ea64d278795f6956683d33d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Quadratic with known spectrum: L=10, μ=1\n",
    "L, mu = 10.0, 1.0  # Lipschitz and strong convexity\n",
    "Q = np.diag([L, mu])  # SPD\n",
    "f = lambda v: 0.5*float(v @ Q @ v)  # objective\n",
    "g = lambda v: Q @ v  # gradient\n",
    "alpha = 0.9 / L  # safe step (<2/L)\n",
    "# Descent-lemma bound and monotone decrease\n",
    "x = np.array([2.0, -1.5], float); ok=True; vals=[]\n",
    "for _ in range(40):\n",
    "    gv = g(x); fx = f(x)\n",
    "    x_next = x - alpha * gv; fx_next = f(x_next)\n",
    "    bound = fx - alpha*(1 - 0.5*L*alpha)*float(gv @ gv)\n",
    "    ok &= (fx_next <= bound + 1e-12)  # bound holds\n",
    "    vals.append(fx_next); x = x_next\n",
    "mono = all(vals[i+1] <= vals[i] + 1e-12 for i in range(len(vals)-1))\n",
    "# Linear contraction vs theory ρ\n",
    "rho = max(abs(1 - alpha*mu), abs(1 - alpha*L))\n",
    "x = np.array([2.0, -1.5], float); ratios=[]\n",
    "for _ in range(40):\n",
    "    x_prev = x.copy(); x = x - alpha * g(x)\n",
    "    if np.linalg.norm(x_prev) > 0:\n",
    "        ratios.append(\n",
    "            np.linalg.norm(x) / np.linalg.norm(x_prev)\n",
    "        )\n",
    "emp = max(ratios) if ratios else 0.0\n",
    "print('Descent lemma:', ok, '| Monotone:', mono, '| Emp ≤ rho:', emp <= rho + 1e-12)\n",
    "print(f\"rho_theory={rho:.3f}  emp_max={emp:.3f}\")\n"
   ],
   "id": "89a75ec4d6c0435dadf56a9f0aa7fc89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armijo on a 1D slice"
   ],
   "id": "47737c1b717f4641a83aa1e4c7927cec"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def func_1d(x):  # quartic slice\n",
    "    return 0.1*x**4 - 1.5*x**2\n",
    "\n",
    "def grad_1d(x):  # derivative\n",
    "    return 0.4*x**3 - 3.0*x\n",
    "\n",
    "x0 = 4.0  # start\n",
    "g0 = grad_1d(x0); p = -g0  # downhill\n",
    "t = np.linspace(0, 1.4, 400)  # steps\n",
    "phi = func_1d(x0 + t*p); phi0 = func_1d(x0)  # loss\n",
    "c, beta = 0.2, 0.7  # Armijo params\n",
    "armijo = phi0 + c*t*p*g0  # linear bound\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))  # panels\n",
    "ax[0].plot(t, phi, label=r'$\\phi(t)$')  # loss\n",
    "ax[0].plot(t, armijo, 'r--', label='Armijo bound')  # bound\n",
    "# simple backtracking marks\n",
    "tt = 1.0\n",
    "for _ in range(5):\n",
    "    if func_1d(x0 + tt*p) <= phi0 + c*tt*p*g0:  # accepted\n",
    "        ax[0].plot([tt], [func_1d(x0+tt*p)], 'o', color='#1b998b'); break\n",
    "    ax[0].plot([tt], [func_1d(x0+tt*p)], 'x', color='#e84855'); tt *= beta\n",
    "ax[0].legend(); ax[0].set_xlabel('t'); ax[0].set_ylabel(r'$\\phi(t)$')\n",
    "\n",
    "ax[1].plot(t, phi, label='loss')  # loss\n",
    "c_strict, c_len = 0.5, 0.05  # two c\n",
    "ax[1].plot(t, phi0 + c_strict*t*p*g0, 'm--', label='strict')\n",
    "ax[1].plot(t, phi0 + c_len*t*p*g0, 'c--', label='lenient')\n",
    "ax[1].legend(); ax[1].set_xlabel('t')\n",
    "plt.tight_layout(); plt.show()\n"
   ],
   "id": "b88c7312502642e3b6fdd912f2cad82c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "3f55065e26fb42c490089fff2cef1887"
  }
 ]
}