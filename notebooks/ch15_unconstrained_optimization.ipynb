{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "61caf8f9839f47cdb47fd1aa37a58e27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Mathematics for Data Science and Machine Learning\n\n(c) Dr. Yves J. Hilpisch | The Python Quants GmbH\n\nAI-powered by GPT-5\n"
   ],
   "id": "5bdc5eca1100487eb6f5cc45f93b3a7c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 — Unconstrained Optimization\n\nThis notebook mirrors the chapter’s key demos: GD paths on an anisotropic quadratic, numerical checks for the descent lemma and linear rate, and a 1D Armijo backtracking slice."
   ],
   "id": "fc72ff12993b4eea999ade0901801351"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\nimport numpy as np  # arrays, RNG\nimport matplotlib.pyplot as plt  # plotting\nplt.style.use('seaborn-v0_8')  # house style\nrng = np.random.default_rng(15)  # seed\n"
   ],
   "id": "abe95feed88640278316f4d7c9bf75c5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anisotropic quadratic: GD paths"
   ],
   "id": "5e13988ab46f4905a715531991e41da5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_Q(L=50.0, m=1.0, theta_deg=30.0):  # SPD with rotation\n    t = np.deg2rad(theta_deg)  # radians\n    R = np.array([[np.cos(t), -np.sin(t)], [np.sin(t), np.cos(t)]])  # rotation\n    return R.T @ np.diag([L, m]) @ R  # rotated diag\n\ndef f_quad(x, Q):  # 0.5 x^T Q x\n    return 0.5 * float(x @ Q @ x)\n\ndef g_quad(x, Q):  # ∇f = Q x\n    return Q @ x\n\ndef backtrack(x, Q, alpha0=0.2, beta=0.5, c=1e-4):  # Armijo backtracking\n    g = g_quad(x, Q); t = alpha0; fx = f_quad(x, Q); gg = float(g @ g)\n    while f_quad(x - t*g, Q) > fx - c*t*gg:  # sufficient decrease\n        t *= beta  # shrink\n    return t  # accepted step\n\ndef gd_path(Q, x0, steps, alpha=None):  # fixed step or backtracking\n    x = x0.astype(float).copy(); xs = [x.copy()]\n    for _ in range(steps):\n        g = g_quad(x, Q)  # gradient\n        t = alpha if alpha is not None else backtrack(x, Q)  # step size\n        x = x - t*g  # update\n        xs.append(x.copy())  # record\n    return np.array(xs)\n\nQ = make_Q(); x0 = np.array([2.0, -1.0])  # test bowl\np_fix = gd_path(Q, x0, steps=28, alpha=0.03)  # fixed step\np_bt = gd_path(Q, x0, steps=28, alpha=None)  # backtracking\n\n# contours covering both paths\nxs = np.linspace(min(p_fix[:,0].min(), p_bt[:,0].min())-0.5,\n                max(p_fix[:,0].max(), p_bt[:,0].max())+0.5, 220)\nys = np.linspace(min(p_fix[:,1].min(), p_bt[:,1].min())-0.5,\n                max(p_fix[:,1].max(), p_bt[:,1].max())+0.5, 220)\nX, Y = np.meshgrid(xs, ys)\nZ = 0.5*(Q[0,0]*X**2 + 2*Q[0,1]*X*Y + Q[1,1]*Y**2)\n\nfig, ax = plt.subplots(1,2, figsize=(10,4), sharex=True, sharey=True)\nfor a, path, col, title in [(ax[0], p_fix, '#e84855', 'Fixed step'), (ax[1], p_bt, '#1b998b', 'Backtracking')]:\n    a.contour(X, Y, Z, levels=12, cmap='viridis')  # contours\n    a.plot(path[:,0], path[:,1], 'o-', color=col, ms=3)  # path\n    a.plot([x0[0]], [x0[1]], 'o', color='k', ms=6)  # start\n    a.set_title(title)  # title\n    a.set_xlabel('$x_1$')  # label\nax[0].set_ylabel('$x_2$')  # y label\nplt.tight_layout(); plt.show()  # render\n"
   ],
   "id": "2705a1ef27884d6a892dd43044945b94"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descent lemma and linear-rate checks (quadratic)"
   ],
   "id": "09c1c2b28ea64d278795f6956683d33d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Quadratic with known spectrum: L=10, μ=1\nL, mu = 10.0, 1.0  # Lipschitz and strong convexity\nQ = np.diag([L, mu])  # SPD\nf = lambda v: 0.5*float(v @ Q @ v)  # objective\ng = lambda v: Q @ v  # gradient\nalpha = 0.9 / L  # safe step (<2/L)\n# Descent-lemma bound and monotone decrease\nx = np.array([2.0, -1.5], float); ok=True; vals=[]\nfor _ in range(40):\n    gv = g(x); fx = f(x)\n    x_next = x - alpha * gv; fx_next = f(x_next)\n    bound = fx - alpha*(1 - 0.5*L*alpha)*float(gv @ gv)\n    ok &= (fx_next <= bound + 1e-12)  # bound holds\n    vals.append(fx_next); x = x_next\nmono = all(vals[i+1] <= vals[i] + 1e-12 for i in range(len(vals)-1))\n# Linear contraction vs theory ρ\nrho = max(abs(1 - alpha*mu), abs(1 - alpha*L))\nx = np.array([2.0, -1.5], float); ratios=[]\nfor _ in range(40):\n    x_prev = x.copy(); x = x - alpha * g(x)\n    if (np.linalg.norm(x_prev)>0): ratios.append(np.linalg.norm(x)/np.linalg.norm(x_prev))\nemp = max(ratios) if ratios else 0.0\nprint('Descent lemma:', ok, '| Monotone:', mono, '| Emp ≤ rho:', emp <= rho + 1e-12)\nprint(f\"rho_theory={rho:.3f}  emp_max={emp:.3f}\")\n"
   ],
   "id": "89a75ec4d6c0435dadf56a9f0aa7fc89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armijo on a 1D slice"
   ],
   "id": "47737c1b717f4641a83aa1e4c7927cec"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def func_1d(x):  # quartic slice\n    return 0.1*x**4 - 1.5*x**2\n\ndef grad_1d(x):  # derivative\n    return 0.4*x**3 - 3.0*x\n\nx0 = 4.0  # start\ng0 = grad_1d(x0); p = -g0  # downhill\nt = np.linspace(0, 1.4, 400)  # steps\nphi = func_1d(x0 + t*p); phi0 = func_1d(x0)  # loss\nc, beta = 0.2, 0.7  # Armijo params\narmijo = phi0 + c*t*p*g0  # linear bound\n\nfig, ax = plt.subplots(1,2, figsize=(10,4))  # panels\nax[0].plot(t, phi, label=r'$\\phi(t)$')  # loss\nax[0].plot(t, armijo, 'r--', label='Armijo bound')  # bound\n# simple backtracking marks\ntt = 1.0\nfor _ in range(5):\n    if func_1d(x0 + tt*p) <= phi0 + c*tt*p*g0:  # accepted\n        ax[0].plot([tt], [func_1d(x0+tt*p)], 'o', color='#1b998b'); break\n    ax[0].plot([tt], [func_1d(x0+tt*p)], 'x', color='#e84855'); tt *= beta\nax[0].legend(); ax[0].set_xlabel('t'); ax[0].set_ylabel(r'$\\phi(t)$')\n\nax[1].plot(t, phi, label='loss')  # loss\nc_strict, c_len = 0.5, 0.05  # two c\nax[1].plot(t, phi0 + c_strict*t*p*g0, 'm--', label='strict')\nax[1].plot(t, phi0 + c_len*t*p*g0, 'c--', label='lenient')\nax[1].legend(); ax[1].set_xlabel('t')\nplt.tight_layout(); plt.show()\n"
   ],
   "id": "b88c7312502642e3b6fdd912f2cad82c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "3f55065e26fb42c490089fff2cef1887"
  }
 ]
}