{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "8f3aa6e8b1eb45efb18aae41a161d96a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Mathematics for Data Science and Machine Learning\n",
    "\n",
    "**© Dr. Yves J. Hilpisch | The Python Quants GmbH**<br>\n",
    "AI-powered by GPT-5.x.\n",
    "\n",
    "# Chapter 22 — Embeddings & Attention as Linear Algebra\n",
    "\n",
    "This notebook mirrors the chapter’s NumPy formulations with tiny, fast checks."
   ],
   "id": "293ba81989364f4b8f00c99c46a35b21"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ],
   "id": "88b75b8ab1d1446eb04e4ccdb883acdf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up NumPy and readable printing.\n"
   ],
   "id": "61991516f8f0406697450a7572d954c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # numerical arrays and linear algebra\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)  # compact, readable array printing\n"
   ],
   "id": "9b058076fb68453f95d58337763fdc75"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stable row-wise softmax (subtract max to avoid overflow).\n"
   ],
   "id": "557a786de42e493289b667e599e891ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_rows(S):  # row-wise softmax helper\n",
    "    S = S - S.max(axis=1, keepdims=True)  # shift by row max for numerical stability\n",
    "    E = np.exp(S)  # elementwise exponential\n",
    "    return E / (E.sum(axis=1, keepdims=True) + 1e-12)  # row-normalize probs\n"
   ],
   "id": "6931ea47157f4fe49ecc76fe5a55cea0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled dot-product attention with optional causal mask.\n"
   ],
   "id": "98d8b1ab309c4e55bb3f3c09f1675b2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V, causal=False):  # scaled dot-product attention\n",
    "    d = Q.shape[1]  # key/query dimension d\n",
    "    S = (Q @ K.T) / np.sqrt(d)  # scaled similarity scores\n",
    "    if causal:  # apply causal mask if requested\n",
    "        n = S.shape[0]  # sequence length n\n",
    "        mask = np.triu(np.ones((n, n), dtype=bool), k=1)  # future positions\n",
    "        S = S.copy()  # avoid modifying S in-place\n",
    "        S[mask] = -1e9  # push future positions to ~zero prob\n",
    "    A = softmax_rows(S)  # attention weights (row-stochastic)\n",
    "    O = A @ V  # output: weighted sum of values\n",
    "    return O, A  # return output and weights\n"
   ],
   "id": "da22251fe1374223a3440e9e1a73c3bc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example to sanity-check attention shapes and masking.\n"
   ],
   "id": "19a5e117c33841a4b19ffb33dc0f5a12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.default_rng(22)  # reproducible random generator\n",
    "n, d, dv = 6, 4, 2  # sequence length, model dim, value dim\n",
    "X = rs.normal(size=(n, d))  # toy inputs (n×d)\n",
    "Wq = rs.normal(size=(d, d))  # query weight (d×d)\n",
    "Wk = rs.normal(size=(d, d))  # key weight (d×d)\n",
    "Wv = rs.normal(size=(d, dv))  # value weight (d×dv)\n",
    "Q, K, V = X @ Wq, X @ Wk, X @ Wv  # project to Q, K, V\n",
    "O_nc, A_nc = attention(Q, K, V, causal=False)  # non-causal attention\n",
    "O_c, A_c = attention(Q, K, V, causal=True)  # causal attention\n",
    "print('rowsum(non-causal) →', np.round(A_nc.sum(1)[:3], 6))  # rows in A_nc sum to 1\n",
    "print(\n",
    "    'future mass (causal) →',\n",
    "    float(np.triu(A_c, 1).sum())\n",
    ")  # ~0 for causal mask\n"
   ],
   "id": "211931faeaa5407badfa5c7fdc7e52ef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "c2aba108c5ed48019abd21731f2770e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}