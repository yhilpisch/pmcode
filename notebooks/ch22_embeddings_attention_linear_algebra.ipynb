{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "8f3aa6e8b1eb45efb18aae41a161d96a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 22 — Embeddings & Attention as Linear Algebra\n",
    "\n",
    "This notebook mirrors the chapter’s NumPy formulations with tiny, fast checks."
   ],
   "id": "293ba81989364f4b8f00c99c46a35b21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ],
   "id": "9b058076fb68453f95d58337763fdc75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_rows(S):\n",
    "    S = S - S.max(axis=1, keepdims=True)\n",
    "    E = np.exp(S)\n",
    "    return E / (E.sum(axis=1, keepdims=True) + 1e-12)\n"
   ],
   "id": "6931ea47157f4fe49ecc76fe5a55cea0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V, causal=False):\n",
    "    d = Q.shape[1]\n",
    "    S = (Q @ K.T) / np.sqrt(d)\n",
    "    if causal:\n",
    "        n = S.shape[0]\n",
    "        mask = np.triu(np.ones((n, n), dtype=bool), k=1)\n",
    "        S = S.copy(); S[mask] = -1e9\n",
    "    A = softmax_rows(S)\n",
    "    O = A @ V\n",
    "    return O, A\n"
   ],
   "id": "da22251fe1374223a3440e9e1a73c3bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.default_rng(22)\n",
    "n, d, dv = 6, 4, 2\n",
    "X = rs.normal(size=(n, d))\n",
    "Wq = rs.normal(size=(d, d))\n",
    "Wk = rs.normal(size=(d, d))\n",
    "Wv = rs.normal(size=(d, dv))\n",
    "Q, K, V = X @ Wq, X @ Wk, X @ Wv\n",
    "O_nc, A_nc = attention(Q, K, V, causal=False)\n",
    "O_c, A_c = attention(Q, K, V, causal=True)\n",
    "print('rowsum(non-causal) →', np.round(A_nc.sum(1)[:3], 6))\n",
    "print('future mass (causal) →', float(np.triu(A_c,1).sum()))\n"
   ],
   "id": "211931faeaa5407badfa5c7fdc7e52ef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "c2aba108c5ed48019abd21731f2770e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}