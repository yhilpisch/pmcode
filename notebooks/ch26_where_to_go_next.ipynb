{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 26 — Where to Go from Here\n",
    "\n",
    "Tiny gradient-descent demo (NumPy) to keep deps light."
   ],
   "id": "ee391ad233d34a0aa5b5f5dd75cd6453"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rs = np.random.default_rng(26)\n",
    "X = np.linspace(-1, 1, 64)[:, None]\n",
    "y = np.sin(3*X) + 0.1*rs.normal(size=X.shape)\n",
    "# 1-hidden layer tanh network: 1→16→1\n",
    "W1 = rs.normal(size=(1,16)); b1 = np.zeros((16,))\n",
    "W2 = rs.normal(size=(16,1)); b2 = np.zeros((1,))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "for _ in range(200):\n",
    "    H = tanh(X@W1 + b1)\n",
    "    yhat = H@W2 + b2\n",
    "    err = yhat - y\n",
    "    dW2 = H.T @ err / len(X); db2 = err.mean(axis=0)\n",
    "    dH = err @ W2.T\n",
    "    dZ = dH * (1 - H**2)\n",
    "    dW1 = X.T @ dZ / len(X); db1 = dZ.mean(axis=0)\n",
    "    W2 -= 1e-2 * dW2; b2 -= 1e-2 * db2\n",
    "    W1 -= 1e-2 * dW1; b1 -= 1e-2 * db1\n",
    "loss = float((err**2).mean())\n",
    "print('final MSE =', round(loss, 4))\n"
   ],
   "id": "cb408be422f64d9185787ac7b85bbdcd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}