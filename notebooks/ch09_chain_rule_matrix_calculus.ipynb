{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fdae29",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee13471",
   "metadata": {},
   "source": [
    "# Python & Mathematics for Data Science and Machine Learning\n",
    "\n",
    "**© Dr. Yves J. Hilpisch | The Python Quants GmbH**<br>\n",
    "AI-powered by GPT-5.\n",
    "\n",
    "**© Dr. Yves J. Hilpisch | The Python Quants GmbH**<br>\n",
    "AI-powered by GPT-5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d414bd6",
   "metadata": {},
   "source": [
    "# Chapter 9 — Chain Rule at Scale & Matrix Calculus\n",
    "\n",
    "Manual derivatives meet executable checks: we compose layers, push sensitivities backward, and verify analytic Jacobian products numerically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7a2c0",
   "metadata": {},
   "source": [
    "## Imports & styling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02c041",
   "metadata": {},
   "source": [
    "## Manual gradients for a tiny network\n",
    "\n",
    "We mirror the chapter example: a two-layer tanh network trained with squared-error loss, followed by a manual VJP-style backward pass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)  # reproducible parameters\n",
    "W1 = rng.normal(size=(4, 3)).astype(np.float64)\n",
    "b1 = rng.normal(size=4).astype(np.float64)\n",
    "W2 = rng.normal(size=(2, 4)).astype(np.float64)\n",
    "b2 = rng.normal(size=2).astype(np.float64)\n",
    "x = rng.normal(size=3).astype(np.float64)\n",
    "target = rng.normal(size=2).astype(np.float64)\n",
    "\n",
    "def forward(W1, b1, W2, b2, x):\n",
    "    \"\"\"Two-layer network with tanh hidden and L2 loss.\"\"\"\n",
    "    a1 = W1 @ x + b1\n",
    "    h = np.tanh(a1)\n",
    "    z = W2 @ h + b2\n",
    "    residual = z - target\n",
    "    loss = 0.5 * np.sum(residual**2)\n",
    "    return loss, (a1, h, z, residual)\n",
    "\n",
    "def backward(W1, b1, W2, b2, x, cache):\n",
    "    \"\"\"Manual backprop that mirrors the algebra in the chapter.\"\"\"\n",
    "    a1, h, z, residual = cache\n",
    "    grad_z = residual  # derivative of 0.5||z-target||^2\n",
    "    grad_W2 = grad_z[:, None] * h[None, :]\n",
    "    grad_b2 = grad_z\n",
    "    grad_h = W2.T @ grad_z\n",
    "    grad_a1 = grad_h * (1.0 - h**2)  # tanh' = 1 - tanh^2\n",
    "    grad_W1 = grad_a1[:, None] * x[None, :]\n",
    "    grad_b1 = grad_a1\n",
    "    grad_x = W1.T @ grad_a1\n",
    "    return grad_W1, grad_b1, grad_W2, grad_b2, grad_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6629801",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, cache = forward(W1, b1, W2, b2, x)\n",
    "grads = backward(W1, b1, W2, b2, x, cache)\n",
    "\n",
    "print(f'loss: {loss:.4f}')\n",
    "for label, grad in zip(['dW1', 'db1', 'dW2', 'db2', 'dx'], grads):\n",
    "    print(f'{label} shape: {grad.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diff(param, loss_fn, eps=1e-5):\n",
    "    \"\"\"Central-difference gradient for every entry in param.\"\"\"\n",
    "    grad = np.zeros_like(param)\n",
    "    it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        original = param[idx]\n",
    "        param[idx] = original + eps\n",
    "        loss_plus, _ = loss_fn()\n",
    "        param[idx] = original - eps\n",
    "        loss_minus, _ = loss_fn()\n",
    "        grad[idx] = (loss_plus - loss_minus) / (2 * eps)\n",
    "        param[idx] = original\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def loss_only():\n",
    "    \"\"\"Closure that recomputes the loss with current parameters.\"\"\"\n",
    "    return forward(W1, b1, W2, b2, x)\n",
    "\n",
    "checks = [\n",
    "    np.allclose(finite_diff(W1, loss_only), grads[0], atol=1e-6),\n",
    "    np.allclose(finite_diff(b1, loss_only), grads[1], atol=1e-6),\n",
    "    np.allclose(finite_diff(W2, loss_only), grads[2], atol=1e-6),\n",
    "    np.allclose(finite_diff(b2, loss_only), grads[3], atol=1e-6),\n",
    "    np.allclose(finite_diff(x, loss_only), grads[4], atol=1e-6),\n",
    "]\n",
    "print('All finite-difference checks pass:', all(checks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e448d93",
   "metadata": {},
   "source": [
    "## JVP vs VJP sanity checks\n",
    "\n",
    "We compare forward- and reverse-mode products on a tanh layer, matching the analytic Jacobian against symmetric finite differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "W = rng.normal(size=(4, 3)).astype(np.float64)\n",
    "x = rng.normal(size=3).astype(np.float64)\n",
    "v = rng.normal(size=3).astype(np.float64)  # forward-mode seed\n",
    "u = rng.normal(size=4).astype(np.float64)  # reverse-mode seed\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Tanh layer used in the chapter's JVP/VJP discussion.\"\"\"\n",
    "    return np.tanh(W @ x)\n",
    "\n",
    "def jvp_exact(x, v):\n",
    "    \"\"\"Analytic JVP = J(x) @ v.\"\"\"\n",
    "    preact = W @ x\n",
    "    diag = 1.0 - np.tanh(preact) ** 2\n",
    "    J = diag[:, None] * W\n",
    "    return J @ v\n",
    "\n",
    "def vjp_exact(x, u):\n",
    "    \"\"\"Analytic VJP = u^T J(x).\"\"\"\n",
    "    preact = W @ x\n",
    "    diag = 1.0 - np.tanh(preact) ** 2\n",
    "    J = diag[:, None] * W\n",
    "    return u @ J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6decff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5  # central-difference step\n",
    "jvp_fd = (f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "\n",
    "phi = lambda z: np.dot(u, f(z))  # scalar projection for the VJP check\n",
    "vjp_fd = np.zeros_like(x)\n",
    "for i in range(x.size):  # sweep standard basis directions\n",
    "    e_i = np.zeros_like(x)\n",
    "    e_i[i] = eps\n",
    "    vjp_fd[i] = (phi(x + e_i) - phi(x - e_i)) / (2 * eps)\n",
    "\n",
    "print('JVP matches:', np.allclose(jvp_fd, jvp_exact(x, v), atol=1e-7))\n",
    "print('VJP matches:', np.allclose(vjp_fd, vjp_exact(x, u), atol=1e-7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57bf2a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "These experiments echo the chapter: stitch Jacobians together analytically, then use compact numerical probes to keep implementations honest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://theaiengineer.dev/tae_logo_gw_flat.png' alt='The Python Quants' width='35%' align='right'>\n"
   ],
   "id": "084d50cd1c124d798829406466824d59"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}